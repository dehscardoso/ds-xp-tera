{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d741aa",
   "metadata": {},
   "source": [
    "<img src = \"https://images2.imgbox.com/c1/79/4H1V1tSO_o.png\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee2330e",
   "metadata": {},
   "source": [
    "# √ÅRVORES DE DECIS√ÉO\n",
    "---\n",
    "\n",
    "## Entendendo problemas lineares e n√£o-lineares\n",
    "---\n",
    "\n",
    "Quando aprendemos, no come√ßo do curso, sobre regress√£o linear vimos que esse modelo funciona muito bem quando temos uma certa linearidade dos nossos dados.\n",
    "\n",
    "Por exemplo, suponha que voc√™ trabalha em um restaurante e quer estimar quanto de gorjeta os seus gar√ßons v√£o receber com base na conta final do restaurante. Logo, quanto maior a conta do restaurante, maior a gorjeta.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/2a/65/R3YKFXEs_o.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73690ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr \n",
    "corr = pearsonr(data1, data2)\n",
    "print(\"Correla√ß√£o de Pearson: %.3f\", corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94276a73",
   "metadata": {},
   "source": [
    "Correla√ß√£o de Pearson: 0.887"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1982c6d",
   "metadata": {},
   "source": [
    "Relembrando um pouco, correla√ß√£o √© um valor que varia entre ‚àí1 at√© 1, em que 1 √© uma correla√ß√£o positiva perfeita e -1 √© uma correla√ß√£o negativa perfeita. Geralmente, um valor acima de 0.50 est√° associado com uma alta correla√ß√£o positiva.\n",
    "\n",
    "\n",
    "Vamos treinar um modelo de regress√£o para esse caso :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce1b4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1.reshape(-1, 1), data2.reshape(-1, 1), test_size=0.1)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "r2_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7623609",
   "metadata": {},
   "source": [
    "0.7238562525812482"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db53dc94",
   "metadata": {},
   "source": [
    "Conseguimos um modelo de regress√£o com o r¬≤ muito bom :) \n",
    "\n",
    "### Podemos at√© fazer um plot da reta em rela√ß√£o aos dados:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/03/a0/V26UD1DI_o.png\" width=\"500\">\n",
    "\n",
    "Contudo, vamos supor que **aconteceu uma doideira no restaurante por um tempo** e a rela√ß√£o das vari√°veis mudaram um pouco, seguindo o gr√°fico abaixo:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/20/fe/PAxG3z7T_o.png\" width=\"500\">\n",
    "\n",
    "Ou seja, a rela√ß√£o claramente deixou de ser claramente linear. Vamos computar a correla√ß√£o de Pearson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3238b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr, _ = pearsonr(data1, data2)\n",
    "print(\"Correla√ß√£o de Pearson: %.3f\" % corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16db7e5",
   "metadata": {},
   "source": [
    "Correla√ß√£o de Pearson: 0.738"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba4eec",
   "metadata": {},
   "source": [
    "A correla√ß√£o ainda √© forte, mas √© claramente menor do que a que calculamos inicialmente. Vamos ver o comportamento da regress√£o nesse caso?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2044b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data1.reshape(-1, 1), data2.reshape(-1, 1), test_size=0.1)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "r2_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdd5029",
   "metadata": {},
   "source": [
    "0.44411006406707687"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac4f34",
   "metadata": {},
   "source": [
    "Ou, seja, claramente a regress√£o n√£o foi muito boa. Vamos plotar a reta para esse caso:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/2d/b8/tsVA2qwp_o.png\" width=\"500\">\n",
    "\n",
    "Como a **regress√£o linear** resulta em uma **reta**, ser√° que conseguimos fazer melhor?\n",
    "\n",
    "\n",
    "# Introduzindo √°rvores de decis√£o\n",
    "---\n",
    "\n",
    "**√Årvores de Decis√£o** s√£o modelos n√£o lineares e que, portanto, conseguem encontrar padr√µes que possuem esse tipo de rela√ß√£o entre as vari√°veis. Mas como ela funciona?\n",
    "\n",
    "A ideia √© relativamente simples. Uma √°rvore de decis√£o √© constru√≠da ao **repartir** os dados seguindo regras de decis√£o. Cada dado, uma vez repartido, resulta em um subgrupo, definido como n√≥. Por exemplo:\n",
    "\n",
    "**Se o valor da conta for maior que 20 reais e menor do que 60, ent√£o o valor a gorjeta deve ser 1.**\n",
    "\n",
    "## O algoritmo das √°rvores de decis√£o\n",
    "---\n",
    "\n",
    "Podemos definir um algoritmo de √°rvore de decis√£o seguindo o seguinte o c√≥digo, inspirado no [link](https://mlcourse.ai/articles/topic3-dt-knn/#2.-Decision-Tree):\n",
    "\n",
    "````\n",
    "def monta_arvore(L):\n",
    " cria um n√≥ t\n",
    " se o crit√©rio de parada √© True:\n",
    " defina um modelo preditivo para t\n",
    " else:\n",
    " encontre o melhor split bin√°rio L = L_left + L_right\n",
    " t.left = monta_arvore(L_left)\n",
    " t.right = monta_arvore(L_right)\n",
    " return t\n",
    "````\n",
    "\n",
    "\n",
    "Mas como que essas regras s√£o definidas?\n",
    "\n",
    "## Regression Trees\n",
    "---\n",
    "\n",
    "No caso de √°rvores de decis√£o aplicadas a problemas de regress√£o, o modelo busca **partir os dados** seguindo um crit√©rio de quebra. No caso, queremos agrupar os nossos dados de forma que esses dados resultem em um **grupo que minimizem o erro quadr√°tico m√©dio**. Relembrando a m√©trica, MSE, √© definida por:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/87/9d/ADF1ELQf_o.png\" width=\"300\">\n",
    "\n",
    "**Ou seja, queremos minimizar a m√©dia dos erros elevado ao quadrado!**\n",
    "\n",
    "Vamos ao c√≥digo para ilustrar isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "model = DecisionTreeRegressor(criterion=\"mse\", max_depth=4, min_samples_leaf = 20)\n",
    "model.fit(X_train, y_train)\n",
    "r2_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a802e2e0",
   "metadata": {},
   "source": [
    "0.5566920750334438"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62348b86",
   "metadata": {},
   "source": [
    "Fazendo o plot da ‚Äúreta‚Äù:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/4f/8a/0rbqBFGJ_o.png\" width=\"500\">\n",
    "\n",
    "Ou seja, **claramente** o valor aprendido **n√£o segue uma reta**, mas sim regras mais ‚Äúsecas‚Äù de corte dos nossos dados.\n",
    "\n",
    "````\n",
    "import cv2\n",
    "from pydotplus.graphviz import graph_from_dot_data\n",
    "from sklearn.tree import export_graphvizdot_data = export_graphviz(\n",
    "    model, filled=True, rounded=True,\n",
    "    out_file=None\n",
    ")\n",
    "graph = graph_from_dot_data(dot_data) \n",
    "graph.write_png('tree.png')img = cv2.imread('tree.png')\n",
    "plt.figure(figsize = (20, 20))\n",
    "_ = plt.imshow(img)\n",
    "````\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/dc/3c/g3kpBFTi_o.png\" width=\"900\">\n",
    "\n",
    "N√≥s podemos interpretar o valor acima da seguinte forma:\n",
    "\n",
    "- Se o valor do pre√ßo da conta for **menor** que 21.98, **maior** que 15.96, **ent√£o** o pre√ßo da gorjeta, baseado na m√©dia de 39 amostras, deve ser 0.88\n",
    "\n",
    "- Se o valor do pre√ßo da conta for **maior** que 21.98, **maior** que 60.0 e **menor** que 79.6, **menor do que** 65.18, **ent√£o** o pre√ßo da gorjeta, baseado na m√©dia de 21 amostras, deve ser 1.646\n",
    "\n",
    "## Desafios de √°rvores de decis√£o\n",
    "---\n",
    "\n",
    "Claramente, poder√≠amos construir uma √°rvore de decis√£o em que os **n√≥s** possuissem **apenas** uma **√∫nica amostra**. Contudo, isso claramente resultaria em √°rvore overfitada e esse √© o maior problema das √°rvores de decis√£o: **elas overfittam** muito f√°cil e por conta disso eu coloquei alguns **hiperpar√¢metros** quando instanciei as classe DecisionTreeRegressor. Al√©m do crit√©rio da √°rvore, os principais hiperpar√¢metros das √°rvores de Decis√£o s√£o:\n",
    "\n",
    "- max_depth- a profundidade m√°xima da √°rvore (no nosso exemplo, usamos 3)\n",
    "\n",
    "- max_features ‚Äî o n√∫mero **m√°ximo** de features que o modelo olha para escolher a melhor regra de decis√£o\n",
    "\n",
    "- min_samples_leaf ‚Äî o n√∫mero m√≠nimo de amostras que queremos ter na √°rvore. No caso, usamos 20.\n",
    "\n",
    "## Classification Trees\n",
    "---\n",
    "\n",
    "\n",
    "Como √Årvores de Decis√£o funcionam no caso de problemas de classifica√ß√£o?\n",
    "\n",
    "A primeira coisa a se pensar, contudo √© que, como **estamos lidando com um problema de classifica√ß√£o**, n√£o podemos ter MSE como crit√©rio de divis√£o. Para definirmos o crit√©rio ideal, vamos pegar o seguinte problema de classifica√ß√£o:\n",
    "\n",
    "Suponha que voc√™ queira prever a cor de uma bola baseada na sua posi√ß√£o:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/8e/55/iyeWgkQi_o.png\" width=\"800\">\n",
    "\n",
    "Nesse exemplo, temos 9 bolas azuis e 11 bolas amarelas. Se aleatoriamente pegarmos uma bola, ela ser√° azul com a probabilidade 9/20 e amarela com probabilidade 11/20. Com isso, conseguimos ver que a chance de pegar uma bola de cada cor √© mais ou menos a mesma. A partir dessa ideia, precisamos definir um conceito **novo**, chamado **entropia**.\n",
    "\n",
    "## Entropia de Shannon\n",
    "---\n",
    "\n",
    "A entropia de Shannon √© um conceito da **teoria da informa√ß√£o** que define o grau de **caos** de um sistema. Quanto maior a entropia, menos ordenado √© um ambiente. Ela √© uma m√©trica que √© definida da seguinte forma:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/f8/24/pCVOko8T_o.png\" width=\"300\">\n",
    "\n",
    "O resultado da entropia √© medido em bits e pode ser um valor entre 0 e 1, em que 0 seria um sistema bem ordenado, com alto grau de certeza e 1, um alto grau de incerteza. O **log** aqui √© na base do **n√∫mero de classes que estamos usando**. Como no nosso caso, estamos lidando com 2 classes, o log √© na base 2.\n",
    "\n",
    "Graficamente, ela pode ser ilustrada pelo gr√°fico abaixo:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/be/2d/kajdRHJG_o.png\" width=\"500\">\n",
    "\n",
    "Ou seja, segundo o exemplo, quando a probabilidade de pertencer a classe positva √© 1 (apenas exemplos positivos), ou 0 (apenas exemplos negativos), a entropia resultante √© 0, porqu√™ **existe uma alta certeza da organiza√ß√£o do ambiente**.\n",
    "\n",
    "Disso, conseguimos derivar outra m√©trica, que √© o ganho de informa√ß√£o:\n",
    "\n",
    "ùêºùê∫(ùëå,ùëã)=ùê∏(ùëã)‚àíùê∏(ùëå|ùëã)\n",
    "\n",
    "O ganho de informa√ß√£o de Y para X √© dado pela Entropia de X menos a Entropia de Y dado X. **Na pr√°tica, quanto maior redu√ß√£o de incertezas, maior o ganho de informa√ß√£o poss√≠vel**. Ou seja, existe uma rela√ß√£o **inversa** entre a entropia e o ganho de informa√ß√£o.\n",
    "\n",
    "No caso, a entropia do sistema que estamos lidando √© dado por:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/44/67/4UiIIbxa_o.png\" width=\"500\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/e0/d9/I1lQ8qeC_o.png\" width=\"800\">\n",
    "\n",
    "Logo, o nosso Y √© basicamente dado pelo sistema pela regra de decis√£o Y ‚â§ 12. E X √© o sistema todo, sem nenhuma regra de quebra at√© ent√£o (E(X)=H_0).\n",
    "\n",
    "O grupo da esquerda tem 13 bolas, sendo 8 azuis e 5 amarelas, tendo como entropia H_1 e o grupo da direita tem 7 bolas, sendo 6 amarelas e 1 azul, tendo a entropia H_2.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/b9/5d/TytOxTQW_o.png\" width=\"400\">\n",
    "\n",
    "Ent√£o, podemos calcular o ganho de informa√ß√£o dessa parte do nosso dataset da seguinte forma:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/92/e2/Uh4qDz1h_o.png\" width=\"500\">\n",
    "\n",
    "Ou seja, o uso da regra de decis√£o ùëã‚â§12 resulta em um sistema mais ordenado que o anterior, uma vez que existem algum ganho de informa√ß√£o.\n",
    "\n",
    "Repetimos, ent√£o, o processo:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/75/85/7NHrbt1b_o.png\" width=\"900\">\n",
    "\n",
    "Para o grupo da direita, a gente precisa apenas fazer uma parti√ß√£o a mais, uma vez que temos apenas uma bola na cor azul (basta perguntar se a posi√ß√£o da bola √© menor ou igual que 18). Contudo, para o grupo da esquerda, precisamos repetir o processo tr√™s vezes. **Al√©m disso, tamb√©m √© valido dizer que a entropia de um grupo onde todas as bolas s√£o da mesma cor √© 0.**\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/af/5f/QdHYt2xs_o.png\" width=\"500\">\n",
    "\n",
    "## S√≥ podemos usar a entropia?\n",
    "---\n",
    "\n",
    "N√£o! Assim como na √°rvore para regress√£o, existem outros crit√©rios de defini√ß√£o al√©m da entropia, como o **Indice de Gini**. A id√©ia √© que a rela√ß√£o entre as m√©tricas seja parecida e isso pode ser visto com facilidade pelo gr√°fico abaixo:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/12/47/qmUMKxIh_o.png\" width=\"500\">\n",
    "\n",
    "Em que o Gini e o missclassification s√£o computadas, respectivamente por:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/c3/86/mZRTQIyg_o.png\" width=\"500\">\n",
    "\n",
    "Na pr√°tica, as diferentes m√©tricas (tendem a convertir para o mesmo resultado). A √∫nica diferen√ßa mais ‚Äúpr√°tica‚Äù √© que o √≠ndice de Gini √© computacionalmente mais eficiente do que a entropia por n√£o ter o c√°lculo do log. Al√©m disso, se multiplicarmos o √≠ndice de Gini por 2, temos uma curva praticamente id√™ntica √† entropia.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/cf/5e/NIlAsiFl_o.png\" width=\"500\">\n",
    "\n",
    "## Aplicando um Exemplo\n",
    "---\n",
    "\n",
    "Suponha que queremos criar um modelo que separe corretamente as classes abaixo:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/8e/d6/TT0Ez7bh_o.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd45f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=3) # training the tree\n",
    "clf_tree.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe3d0a9",
   "metadata": {},
   "source": [
    "Podemos imprimir a **fronteira de decis√£o**\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/78/a2/L8Qt23Bq_o.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68aaf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(\n",
    "    clf_tree, filled=True, rounded=True,\n",
    "    out_file=None\n",
    ")\n",
    "graph = graph_from_dot_data(dot_data) \n",
    "graph.write_png('tree_classifier.png')\n",
    "img = cv2.imread('tree_classifier.png')\n",
    "plt.figure(figsize = (20, 20))\n",
    "_ = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59a8ae0",
   "metadata": {},
   "source": [
    "<img src = \"https://images2.imgbox.com/64/33/cis0NkrT_o.png\" width=\"800\">\n",
    "\n",
    "## Um gostinho do que vem por a√≠\n",
    "---\n",
    "\n",
    "O interessante das √Årvores de Decis√£o √© que eles s√£o a base para muitos algoritmos populares, como **Random Forests** e **Gradient Boosting**, que, por sua vez, s√£o bastante utilizados na pr√°tica.\n",
    "\n",
    "O interessante desses algoritmos √© que eles s√£o capazes de capturar n√£o linearidades de uma maneira melhor. Pegando o exemplo da gorjeta, que tal compararmos uma √°rvore de decis√£o comum (o exemplo no come√ßo do post) e uma random forest.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/84/25/fKteavyL_o.png\" width=\"600\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/83/d6/dGUdHyvJ_o.png\" width=\"500\">\n",
    "\n",
    "\n",
    "## Conclus√£o\n",
    "\n",
    "Parab√©ns! Voc√™s acabaram de aprender um pouco mais sobre um dos modelos mais cl√°ssicos de machine learning, as **Decision Trees** :)\n",
    "\n",
    "## Refer√™ncias\n",
    "\n",
    "https://towardsdatascience.com/https-medium-com-lorrli-classification-and-regression-analysis-with-decision-trees-c43cdbc58054\n",
    "\n",
    "https://mlcourse.ai/articles/topic3-dt-knn/#2.-Decision-Tree\n",
    "\n",
    "https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8\n",
    "\n",
    "https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8ed46e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
