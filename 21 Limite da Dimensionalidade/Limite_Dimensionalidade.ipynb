{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c803b54",
   "metadata": {},
   "source": [
    "<img src = \"https://images2.imgbox.com/c1/79/4H1V1tSO_o.png\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ce0c07",
   "metadata": {},
   "source": [
    "# Qual o limite da dimensionalidade?\n",
    "---\n",
    "\n",
    "Que bom que você está aqui. Espero que tenha conseguido treinar um pouco de tudo o que vimos e compreender os conceitos por trás das técnicas e algoritmos que vimos até aqui. A construção do arsenal de qualquer Data Scientists passa por uma primeira etapa de familiaridade e ganha uma qualidade exponencial com o uso, testes, ajustes. Ou seja, tentem experimentar um pouco de tudo o que já vimos. Esse é o momento de aprender. Compartilhe as dificuldades e aprendizados no grupo do slack e você vai ver a mágica do conhecimento e aprendizado coletivo em ação. Nos últimos textos falamos bastante sobre como tornar o modelo mais estável e complexo, seja pela engenharia de features ou pelo agrupamento de algoritmos. Estamos falando mais especificamente tanto de Feature Engineering quanto de Ensembles. \n",
    "\n",
    "Neste texto vamos falar de Redução de dimensionalidade, algo que pode parecer contraditório em relação a tudo o que vimos em Feature Engineering. Mas essas técnicas de aprendizado não supervisionado são fundamentais na caixa de ferramentas de profissionais de ciência de dados. \n",
    "\n",
    "\n",
    "## A maldição da dimensionalidade\n",
    "---\n",
    "\n",
    " Já falamos um pouco sobre o conceito de dimensionalidade e redução de dimensionalidade no texto Learning>Machine, que abordava os temas de aprendizado supervisionado e não supervisionado. Nele, vimos que cada nova feature ou coluna representa uma dimensão. Isso pode causar problemas nos mais diversos aspectos como: \n",
    "\n",
    "- A dificuldade de visualizar objetos com mais dimensões do que 3:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/6b/c5/xqgeQhoD_o.png\" width=\"500\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- O aumento da complexidade e necessidade computacional, tanto para processar, quanto para armazenar esses dados\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/2e/30/6zgsTHp2_o.png\" width=\"800\">\n",
    "\n",
    "# Principais problemas causados nos algoritmos \n",
    "---\n",
    "\n",
    "## Alta complexidade\n",
    "---\n",
    "\n",
    "Imagine que o seu dataset possui em uma feature mais de 50 categorias diferentes. Quando isto acontece, dizemos que esta coluna possui alta cardinalidade. Se decidirmos mantê-la, ao passarmos pelo one-hot encoding, automaticamente nosso dataset passou a ter mais 50 (ou 49 se usarmos o conceito de dummies) dimensões. Sabendo que estamos falando do acréscimo do número de observações elevado à 50ª potência, intuitivamente já percebemos que não é uma boa ideia treinar um modelo e fazer previsões com um dataset tão complexo quanto este.\n",
    "\n",
    "## Esparsidade dos dados\n",
    "---\n",
    "\n",
    "Um outro problema comum é que os dados gerados são muito esparsos. Imagine que em 50 colunas, teremos 49 zeros e apenas 1 número um. Isto é difícil para diversos algoritmos calcular e estabelecer uma significância estatística. Em outras palavras, seria necessário um conjunto muito grande de observações o que tornaria ainda mais complexo o dataset.\n",
    "\n",
    "## Decorar x Aprender\n",
    "---\n",
    "\n",
    "Vamos supor que tomamos a decisão de usar um dataset com um número elevado de dimensões. Existe uma grande probabilidade que pouquíssimas observações tenham a mesma combinação de features. Por exemplo em um dataset de carros, onde precisaríamos prever o seu valor de venda baseado em suas features, se tivéssemos 30 features como: cor, marca, modelo, ano, com_adesivo, com_roda, com_defeito_maçaneta, mais_50km… provavelmente chegaríamos à combinações em que teríamos no máximo 2 ou 3 observações. E isso, ao invés de ajudar os algoritmos a aprender para generalizar posteriormente, os leva a um overfitting.\n",
    "\n",
    "## Estratégias para resolver a alta dimensionalidade\n",
    "---\n",
    "\n",
    "Agora que já falamos dos perigos da alta dimensionalidade, vamos entender as estratégias e técnicas que podemos usar para tratar, combater e/ou minimizar seus efeitos. Basicamente estamos falando de duas estratégias ou abordagens. Na primeira vamos investigar quais features realmente fazem a diferença e ajudam o algoritmo a aprender, descartando as demais. Na segunda, gerar novas features que são uma derivação das iniciais, capturam informações delas, mas não possuem mais as mesmas interpretações. Veja a imagem abaixo para ter um pouco da intuição por trás desses dois conceitos.\n",
    " \n",
    "<img src = \"https://images2.imgbox.com/2b/19/2EBeHIr9_o.png\" width=\"800\">\n",
    "\n",
    "Exemplo de Feature Selection onde escolhemos algumas features em detrimento à outras. Pode haver uma perda sensível de informações.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/ce/0b/0m6ETaYp_o.png\" width=\"800\">\n",
    "\n",
    "## Feature selection\n",
    "---\n",
    "\n",
    "Existem algumas técnicas que podemos usar para realizar uma feature selection. Em geral, é importante que haja um conhecimento sobre o dataset e o problema de negócio que você está atacando. Caso contrário, pode ser que exclua alguma feature importante que vai fazer falta no modelo em produção, por exemplo. \n",
    "\n",
    "## Seleção por correlação\n",
    "---\n",
    "\n",
    " o fazer uma análise exploratória podemos identificar visualmente duas ou mais variáveis que são absolutamente correlacionadas. Muitas vezes pode ser uma transformação de dados ou uma outra forma de derivada de capturar a mesma informação. O fato é que ao variarem de uma maneira igual, elas registram exatamente a mesma informação ao dataset. Para exemplificar, pegamos o dataset Ansur com informações de homens e mulheres das forças armadas americanas. Vamos pegar um subset do dataset com apenas 3 features originais (altura, peso em kg e gênero) e 2 features criadas a partir delas (peso em libras e o IMC). Vamos enxergar como essas correlações ficam visivelmente claras:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/7a/b9/woibm5VB_o.png\" width=\"800\">\n",
    "<img src = \"https://images2.imgbox.com/65/e3/Ah7aIa8n_o.png\" width=\"500\">\n",
    "\n",
    "Precisamos excluir uma delas (nesse caso, vamos fazer um drop na coluna do peso em libras e manter o peso em kg, por uma questão de familiaridade nossa com a medida):\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/44/c7/tkZ6hHX9_o.png\" width=\"700\">\n",
    "\n",
    "Dependendo da análise que se vai fazer, é possível manter apenas o IMC e descartar as duas features de peso e altura, já que estas últimas foram capturadas quase em sua totalidade pelo IMC. Mas existem outras formas mais computacionais e estatísticas de fazer uma feature selection.\n",
    "\n",
    "## Seleção de features por variância \n",
    "---\n",
    "\n",
    "Como já vimos, a variância é um dos elementos que mais impacta na qualidade e performance dos modelos. Logo, features com maiores variâncias vão impactar mais e ser mais relevantes para o nosso modelo do que features com menos variância. Dessa vez, vamos usar todas as features numéricas do dataset Ansur.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/e8/73/UqyIaBmd_o.png\" width=\"700\">\n",
    "\n",
    "Um ponto importante para que essa seleção por threshold tenha efeito é que todos os dados estejam normalizados. Isso porque precisamos capturar apenas a variação dentro de cada coluna as relações entre das diferentes escalas. Por exemplo, o tamanho da mão é muito menor em centímetros que o tamanho da tíbia, mas ao normalizar, preservamos apenas a variância dentro de cada coluna. Para normalizar as features, basta dividirmos o valor da observação pela média da coluna em questão. Vamos aos cálculos: \n",
    "\n",
    "**Importando VarianceThreshold** \n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold \n",
    "\n",
    "**Criando VarianceThreshold feature selector com 0.005 **\n",
    "\n",
    "sel = VarianceThreshold(threshold=0.005) \n",
    "\n",
    "**Fitando o selector ao df numeric normalizado**\n",
    "\n",
    "sel.fit(df_numeric / df_numeric.mean())\n",
    "\n",
    "**Criando uma máscara de boleanos**\n",
    "\n",
    "mask = sel.get_support()\n",
    "\n",
    "**Apply the mask to create a reduced dataframe** \n",
    "\n",
    "reduced_ansur = df_numeric.loc[:, mask]\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/33/ba/xpi4BksI_o.png\" width=\"800\">\n",
    "\n",
    "\n",
    "## Seleção de features por performance do modelo \n",
    "---\n",
    "\n",
    "Uma das formas mais intuitivas de escolher as features em machine learning é através do impacto delas no modelo que estamos usado. Para este exemplo vamos usar o dataset Pima Indians editado. Nele temos 8 features com as pré-condições médicas de 392 pacientes e o objetivo é prever na coluna test se o paciente terá ou não diabetes. Para que possamos prever de forma precisa, vamos usar o StandardScaler() que vai padronizar os dados centrando a média de todas as features em 0 e a variância máxima em 1. Vamos fazer as importações necessárias e criar os datasets de treino e test.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/fc/0e/3WIzQCi7_o.png\" width=\"1000\">\n",
    "\n",
    "**Instanciando o StandardScaler e a Regressão logística**\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "lr = LogisticRegression() \n",
    "\n",
    "**Vamos fittar e transformar os dados de uma só vez** \n",
    "\n",
    "X_train_std = scaler.fit_transform(X_train) \n",
    "\n",
    "**Treinando a regressão linear no X_train padronizado**\n",
    "\n",
    "lr.fit(X_train_std, y_train) \n",
    "\n",
    "**Precisamos transformar os dados de test também**\n",
    "\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "**Fazendo predições com os dados padronizados**\n",
    "\n",
    "y_pred = lr.predict(X_test_std) \n",
    "\n",
    "**Imprimindo as métricas de acurácia e os coeficientes de importância das features**\n",
    "\n",
    "print(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred))) \n",
    "\n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed9480",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
