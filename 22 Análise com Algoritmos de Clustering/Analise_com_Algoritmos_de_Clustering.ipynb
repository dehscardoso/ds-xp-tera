{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0bf8733",
   "metadata": {},
   "source": [
    "<img src = \"https://images2.imgbox.com/c1/79/4H1V1tSO_o.png\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962c6334",
   "metadata": {},
   "source": [
    "# Análise com algoritmos de clustering\n",
    "---\n",
    "\n",
    "Que bom que você chegou até aqui! Eu sei que nos últimos textos temos aumentado o grau de complexidade e de autonomia que esperamos de vocês.\n",
    "Temos enfatizado ainda que compartilhe e ajude a criar um senso de comunidade com os seus colegas de curso. Isso acontece, entre outras coisas, porque a cada novo assunto que vamos avançando, os materiais se tornam mais escassos, mais complexos, com menos exemplos de aplicações práticas… Esses elementos podem ser compensados pelo poder do conhecimento coletivo. Pessoas que testaram ou estudaram por horas, dias, semanas, meses e anos, dão um grande ganho à comunidade ao documentar e compartilhar uma implementação de um algoritmo, por exemplo.\n",
    "\n",
    "Neste texto vamos abordar um assunto que começamos lá no início da nossa jornada. Quando falávamos de algoritmos de aprendizado não supervisionado, nos referíamos principalmente à clusterização e redução de dimensionalidade. O segundo, abordamos no outro texto deste mesmo bloco de estudos. Mas aqui vamos fazer de cluster!\n",
    "\n",
    "# O que é um cluster?\n",
    "---\n",
    "\n",
    "É um grupo de elementos ou observações com características similares entre si. Sua identificação é possível, graças a combinação de fatores que se repete em cada conjunto de observações similares. Mas, diferente dos algoritmos supervisionados, em que criávamos labels para indicar o padrão que estávamos procurando, os algoritmos não supervisionados são capazes de aprender os padrões sem a necessidade de labels. Como isso acontece? Bem cada algoritmo tem uma forma particular de aprender e identificar os padrões nos dados, mas eles se dividem em 4 grupos ou métodos.\n",
    "\n",
    "# Os 4 principais Métodos de Clusterização\n",
    "---\n",
    "\n",
    "Assista a esse vídeo: https://youtu.be/Se28XHI2_xE\n",
    "\n",
    "Agora que conhecemos um pouco da intuição e da descrição de cada um dos grupos, vamos mergulhar um pouco mais nos 4 principais métodos de clusterização (Clusterização por centróides, Clusterização por densidade, Clusterização por distribuição e Clusterização por conecitividade).\n",
    "\n",
    "### Geração de dados sintéticos\n",
    "\n",
    "Para acompanhar um pouco do efeito da clusterização, vamos criar um dataset sintético que vai servir de base para compararmos os diferentes algoritmos em ação.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/bc/36/1kEuaZMX_o.png\" width=\"700\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"700\">\n",
    "\n",
    "# Clusterização baseada em centroids\n",
    "---\n",
    "\n",
    "## K-Means Clustering\n",
    "\n",
    "É um método de clusterização baseado em centróides (K-means), em que cada elemento do cluster tem como base a sua distância média de um dos centros determinados no momento de instanciar o modelo. Por exemplo n_clusters=2. Este número k, definido de antemão, será calculado por uma série de tentativas aleatórias o que minimiza a distância entre pontos no conjunto de dados e cada centro de cluster. O problema é conhecido como NP-hard e, portanto, as soluções são comumente aproximadas ao longo de uma série de tentativas. Porém o output do K-means será sempre catogórico: pertence a determinado cluster ou não sem a possibilidade de ambiguidades ou probabilidades.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "A maior desvantagem dos algoritmos do tipo k-means é que eles exigem que o número de clusters k seja especificado com antecedência. Isso causa problemas para dados de clustering quando o número de clusters não pode ser conhecido de antemão. K-means também tem problemas para agrupar distribuições baseadas em densidade, como aquelas na imagem abaixo ou aquelas que não são lineares separáveis.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "## Passo a passo do algoritmo:\n",
    "---\n",
    "\n",
    "1. Selecione a quantidade de classes que o algoritmo deve buscar, ele inicializa aleatoriamente os centroides. O algoritmo reinicia a busca randômica por centroides a cada round. Os pontos centrais, denotados como X no gráfico, são vetores com o mesmo comprimento de cada vetor de ponto de dados.\n",
    "\n",
    "2. O K-means classifica cada data point calculando a distância entre os pontos específicos e o centro de cada grupo. O próximo passo é classificar os pontos que pertencem ao grupo cujo centro é o mais próximo deles.\n",
    "\n",
    "3. Com base nessas informações, tire a média de todos os vetores no grupo específico e recalcule o centro do grupo.\n",
    "\n",
    "4. Repita o procedimento por um número de vezes e certifique-se de que os centros do grupo não variem muito entre as iterações.\n",
    "\n",
    "### Prós\n",
    "\n",
    "- K-means é um método rápido porque não realiza muitos cálculos.\n",
    "\n",
    "- Não realiza predições ambíguas.\n",
    "\n",
    "### Contras\n",
    "\n",
    "- Identificar e classificar os grupos pode ser um aspecto desafiador.\n",
    "\n",
    "- Como começa com uma escolha aleatória de centros de cluster, os resultados podem ser inconsistentes.\n",
    "\n",
    "## Implementação básica:\n",
    "---\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "## Mean-Shift Clustering\n",
    "---\n",
    "\n",
    "O Mean-Shift Clustering ou agrupamento de deslocamento médio (em uma tradução livre) é um algoritmo baseado em foco deslizante que tenta encontrar áreas densas nos data points. Também é um algoritmo baseado em centróides, o que significa que o objetivo é localizar os pontos centrais de cada grupo / classe. Para fazer isso, ele fica atualizando candidatos a pontos centrais a serem a média dos pontos dentro da desse foco ou janela deslizante. Essas janelas candidatas são filtradas em um estágio de pós-processamento para eliminar as similares, formando o conjunto final de pontos centrais e seus grupos correspondentes. Confira o gráfico abaixo para ver uma ilustração.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "## Mean-Shift Clustering pela perspectiva de uma única janela:\n",
    "---\n",
    "\n",
    "1. Para explicar o mean-shift ou a mudança de média, consideraremos um conjunto de pontos no espaço bidimensional como na ilustração acima. Começamos com uma janela deslizante circular centrada em um ponto C (selecionado aleatoriamente) e tendo o raio r como o núcleo. A mudança média é um algoritmo de escalada que envolve a mudança desse kernel iterativamente para uma região de densidade mais alta em cada etapa até a convergência.\n",
    "\n",
    "2. A cada iteração, a janela deslizante é deslocada para regiões de maior densidade, deslocando o ponto central para a média dos pontos dentro da janela (daí o nome). A densidade dentro da janela deslizante é proporcional ao número de pontos dentro dela. Naturalmente, ao mudar para a média dos pontos na janela, ele se moverá gradualmente em direção às áreas de maior densidade de pontos.\n",
    "\n",
    "3. Continuamos deslocando a janela deslizante de acordo com a média até que não haja uma direção na qual uma mudança possa acomodar mais pontos dentro do kernel. Confira o gráfico acima; continuamos movendo o círculo até que não aumentemos mais a densidade (ou seja, o número de pontos na janela).\n",
    "\n",
    "4. Este processo das etapas 1 a 3 é feito com muitas janelas deslizantes até que todos os pontos fiquem dentro de uma janela. Quando várias janelas deslizantes se sobrepõem, a janela que contém a maioria dos pontos é preservada. Os pontos de dados são então agrupados de acordo com a janela deslizante na qual residem.\n",
    "\n",
    "Uma ilustração do processo inteiro, de ponta a ponta com todas as janelas deslizantes é mostrada abaixo. Cada ponto preto representa o centroide de uma janela deslizante e cada ponto cinza é um ponto de dados.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "## Uma visão gerado de todo o processo de Mean-Shift Clustering\n",
    "---\n",
    "\n",
    "Em contraste com o K-means, não há necessidade de selecionar o número de clusters, pois o Means-Shift descobre isso automaticamente. Essa é uma de suas maiores vantagens. O fato de os centros do cluster convergirem para os pontos de densidade máxima também é bastante desejável, pois é bastante intuitivo e se encaixa bem, no sentido data-driven. Sua maior desvantagem é que definir seleção do tamanho/raio da janela “r”, escolher este valor pode não ser trivial.\n",
    "\n",
    "## Implementação básica:\n",
    "---\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "## Clusterização baseada em densidade\n",
    "---\n",
    "\n",
    "### DBSCAN — Density-Based Spatial Clustering of Applications with Noise\n",
    "\n",
    "O DBSCAN, um algoritmo de agrupamento baseado em densidade, é uma melhoria em relação ao agrupamento Mean-Shift, pois tem vantagens específicas. O gráfico a seguir pode esclarecer o assunto para você.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "1. Ele começa com um ponto de dados inicial aleatório não visitado. A partir daí, todos os pontos dentro de uma distância ‘Epsilon — Ɛ, são classificados como neighborhood points.\n",
    "\n",
    "2. É preciso um número mínimo de pontos na vizinhança para iniciar o processo de clustering. Quando atingidas essas circuntâncias, o data point atual torna-se o primeiro ponto no cluster. Caso contrário, o ponto será rotulado como ‘Ruído’. Em qualquer caso, o ponto atual se torna um ponto visitado.\n",
    "\n",
    "3. Todos os pontos dentro da distância Ɛ tornam-se parte do mesmo cluster. Repita o procedimento para todos os novos pontos adicionados ao grupo de cluster.\n",
    "\n",
    "4. Continue com o processo até visitar e rotular cada ponto dentro da vizinhança Ɛ do cluster.\n",
    "\n",
    "5. Após a conclusão do processo, comece novamente com um novo ponto não visitado, levando assim à descoberta de mais clusters ou de ruído. No final do processo, certifique-se de marcar cada ponto como cluster ou ruído.\n",
    "\n",
    "## Implementação básica:\n",
    "---\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "\n",
    "## Clusterização baseada em distribuição\n",
    "---\n",
    "\n",
    "### Expectation–Maximization (EM) Clustering using Gaussian Mixture Models (GMM)\n",
    "\n",
    "Uma das principais desvantagens do K-Means é seu uso ingênuo ou simplista do K-Means para determinar o centro do cluster. Observando a imagem abaixo, podemos ver porque usar o K-Means não é a melhor maneira de fazer esta escolha. No lado esquerdo, parece bastante óbvio visualmente que existem dois clusters circulares com raios diferentes centrados na mesma média. O K-Means não pode lidar com isso porque os valores médios dos clusters estão muito próximos. Sem falar que o K-Means também falha nos casos em que os clusters não são circulares, novamente porque usa como resultado do a média como centro do cluster.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "GMMs usando EM Clustering Fonte:https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\n",
    "\n",
    "Modelos de Mistura Gaussiana (GMMs) nos dão mais flexibilidade do que K-Means. **Com GMMs, assumimos que os pontos de dados têm distribuição Gaussiana; esta é uma suposição menos restritiva do que dizer que eles são circulares usando a média.** Dessa forma, **temos dois parâmetros para descrever a forma dos clusters: a média e o desvio padrão!** Tomando um exemplo em duas dimensões, isso significa que os clusters podem assumir qualquer tipo de forma elíptica (uma vez que temos um desvio padrão nas direções x e y). Assim, cada distribuição gaussiana é atribuída a um único cluster.\n",
    "\n",
    "Para encontrar os parâmetros do Gaussiano para cada cluster (por exemplo, a média e o desvio padrão), usaremos um algoritmo de otimização chamado Expectation — Maximization (EM). Dê uma olhada no gráfico abaixo como uma ilustração das gaussianas sendo ajustadas aos clusters. Em seguida, podemos prosseguir com o processo de agrupamento de Expectativa-Maximização usando GMMs.\n",
    "\n",
    "1. Semelhante ao cluster K-means, selecionamos o número de clusters e inicializamos aleatoriamente os parâmetros de distribuição gaussiana para cada um deles.\n",
    "\n",
    "2. Com esse background, calcule a probabilidade de cada ponto de dados pertencer a um determinado cluster. Quanto mais próximo o ponto estiver do centro da Gauss, maiores são as chances de ele pertencer ao cluster.\n",
    "\n",
    "3. Com base nesses cálculos, determinamos um novo conjunto de parâmetros para as distribuições gaussianas para maximizar as probabilidades de pontos de dados dentro dos clusters. Usamos uma soma ponderada das posições dos pontos de dados para calcular essas probabilidades. A probabilidade de o ponto de dados pertencer a um determinado cluster é o fator de peso\n",
    "\n",
    "4. Repita os passos 2 e 3 até a convergência onde não há muita variação.\n",
    "\n",
    "### Prós\n",
    "\n",
    "- Há um maior nível de flexibilidade em relação à covariância de cluster nos GMMs em comparação com o agrupamento de K-médias por causa do conceito de desvio padrão.\n",
    "\n",
    "- Como esse conceito usa probabilidade, você tem vários clusters por data points. Portanto, se um determinado data point pertence a dois clusters sobrepostos, podemos defini-lo de forma ainda mais precisa dizendo que pertence A% à Classe 1 e B% à Classe 2.\n",
    "\n",
    "## Implementação básica:\n",
    "---\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "## Clusterização por conectividade\n",
    "----\n",
    "\n",
    "### Agglomerative Hierarchical Clustering\n",
    "\n",
    "Os algoritmos de Hierarchical Clustering ou agrupamento hierárquico se enquadram em 2 categorias: top-down ou bottom-up. Algoritmos bottom-up ou ascendentes tratam cada ponto de dados como um único cluster no início e então mesclam (ou aglomeram) pares de clusters sucessivamente até que todos os clusters tenham sido mesclados em um único cluster que contém todos os pontos de dados. O agrupamento hierárquico bottom-up é, portanto, denominado agrupamento aglomerativo hierárquico ou HAC. Esta hierarquia de clusters é representada como uma árvore (ou dendrograma). A raiz da árvore é o único cluster que reúne todas as amostras, sendo as folhas os aglomerados com apenas uma amostra. Confira o gráfico abaixo para ver uma ilustração antes de passar para as etapas do algoritmo.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "## Funcionamento do Agglomerative Hierarchical Clustering\n",
    "---\n",
    "\n",
    "1. Começamos tratando cada ponto de dados como um único cluster, ou seja, se houver X pontos de dados em nosso conjunto de dados, teremos X clusters. Em seguida, selecionamos uma métrica de distância que mede a distância entre dois clusters. Como exemplo, usaremos o average linkage, que define a distância entre dois clusters como a distância média entre os pontos de dados no primeiro cluster e os pontos de dados no segundo cluster.\n",
    "\n",
    "2. Em cada iteração, combinamos dois clusters em um. Os dois clusters a serem combinados são selecionados como aqueles com a menor ligação média. Ou seja, de acordo com nossa métrica de distância selecionada, esses dois clusters têm a menor distância entre si e, portanto, são os mais semelhantes e devem ser combinados.\n",
    "\n",
    "3. A etapa 2 é repetida até chegarmos à raiz da árvore, ou seja, temos apenas um cluster que contém todos os data points. Desta forma, podemos selecionar quantos clusters queremos no final, simplesmente escolhendo quando parar de combinar os clusters, ou seja, quando pararmos de construir a árvore!\n",
    "\n",
    "O clustering hierárquico não exige que especifiquemos o número de clusters e podemos até selecionar qual número de clusters parece melhor, já que estamos construindo uma árvore. Além disso, o algoritmo não é sensível à escolha da métrica de distância; todos eles tendem a funcionar igualmente bem, enquanto com outros algoritmos de agrupamento, a escolha da métrica de distância é crítica. **Um caso de uso particularmente bom de métodos de agrupamento hierárquico é quando os dados subjacentes têm uma estrutura hierárquica e você deseja recuperar a hierarquia; outros algoritmos de agrupamento não podem fazer isso.*** Essas vantagens do agrupamento hierárquico vêm ao custo de menor eficiência, pois tem uma complexidade de tempo de O (n³), ao contrário da complexidade linear de K-Means e GMM.\n",
    "\n",
    "## Implementação básica:\n",
    "---\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "# Exemplos de usos de algoritmos de clustering:\n",
    "\n",
    "## Sistemas de Diagnóstico:\n",
    "\n",
    "A profissão médica usa o K-Means na criação de sistemas de apoio à decisão médica mais inteligentes, especialmente no tratamento de doenças do fígado.\n",
    "\n",
    "## Motores de Busca:\n",
    "\n",
    "A clusterização é a espinha dorsal dos motores de busca. Quando uma pesquisa é realizada, os resultados da pesquisa precisam ser agrupados e os mecanismos de pesquisa frequentemente usam clustering para fazer isso.\n",
    "\n",
    "## Redes de sensores sem fio\n",
    "\n",
    "O algoritmo de clustering desempenha o papel de localizar os cluster heads, que coletam todos os dados em seu respectivo cluster.\n",
    "\n",
    "# Marketing e Vendas\n",
    "\n",
    "Personalização e targeting em marketing são um grande negócio.\n",
    "Isso é conseguido examinando as características específicas de uma pessoa e compartilhando com ela as campanhas que tiveram sucesso com outras pessoas semelhantes.\n",
    "\n",
    "Qual é o problema: Se sua empresa está tentando obter o melhor retorno sobre seu investimento em marketing, é crucial que você faça o targeting das pessoas de uma maneira correta. Se errar, você corre o risco de não realizar nenhuma venda ou, pior, prejudicar a confiança do cliente na sua empresa.\n",
    "\n",
    "Como os clusters funcionam: Os algoritmos de clustering são capazes de agrupar pessoas com características semelhantes e probabilidade de compra. Depois de ter os grupos, você pode executar testes em cada grupo com uma cópia de marketing diferente que o ajudará a direcionar melhor suas mensagens para eles no futuro.\n",
    "\n",
    "# Implementando 2 dos algoritmos mais comuns:\n",
    "\n",
    "## K-Means Clustering\n",
    "\n",
    "Vamos trabalhar em um problema de segmentação de clientes de varejo. Você pode baixar o conjunto de dados usando este link.\n",
    "\n",
    "O objetivo desse dataset é segmentar os clientes de um varejista com base em seus gastos anuais em diversas categorias de produtos, como leite, mercearia, região, etc. Então, vamos começar!\n",
    "\n",
    "Antes de tudo, vamos importar as bibliotecas:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "Temos os dados de gastos dos clientes em diferentes produtos como Leite, Comestíveis, Congelados, Detergentes, etc. Agora, temos que segmentar os clientes com base nos detalhes fornecidos. Antes de fazer isso, vamos retirar algumas estatísticas relacionadas aos dados:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "Aqui, vemos que há muita variação na magnitude dos dados. Variáveis como Channel e Region têm magnitude baixa, enquanto variáveis como Fresh, Milk, Grocery, etc. têm magnitude maior.\n",
    "\n",
    "Sabemos que o K-Means é um algoritmo baseado em distância. Essas diferenças de magnitude podem criar um problema. Então, é imprescindível primeiro trazer todas as variáveis ​​para a mesma magnitude:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "Agora sim! Vamos inicializar o K-Means:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "Vamos avaliar o quão bem estão os clusters formados. Para fazer isso, vamos calcular a inércia dos clusters. A inércia é um indicativo de quão estáveis os clusters estão. Ou seja, caso eu promova mais n-iterações, quão diferentes os clusters vão ser entre si a cada nova rodada :Vamos avaliar o quão bem estão os clusters formados. Para fazer isso, vamos calcular a inércia dos clusters. A inércia é um indicativo de quão estáveis os clusters estão. Ou seja, caso eu promova mais n-iterações, quão diferentes os clusters vão ser entre si a cada nova rodada:\n",
    "\n",
    "# inertia on the fitted data\n",
    "kmeans.inertia_\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "Obtivemos um valor de inércia de quase 2600. Isso é bom? Não sabemos! Uma forma de avaliar isto é através do método elbow ou cotovelo. Em datasets com um número de clusters muito claros, o método registra uma queda significativa na inércia, formando um cotovelo no gráfico. Mais ou menos assim:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "Agora, vamos ver como podemos usar a curva de cotovelo para determinar o número ideal de clusters no nosso dataset.\n",
    "\n",
    "Vamos fazer um loop for para fittar diversos modelos KMeans, e, a cada modelo vamos aumentar o número de clusters. Vamos armazenar o valor de inércia de cada modelo e, em seguida, plotá-lo para visualizar o resultado:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "Então podemos escolher qualquer número de clusters entre 5 e 8. Vamos definir o número de clusters como 6 e ajustar o modelo:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "Agora que temos os clusters, o que os diferencia?\n",
    "\n",
    "Desafio: faça uma EDA e tente justificar as diferenças entre os diversos clusters.\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "Hierarchical Clustering\n",
    "\n",
    "Os clusters hierárquicos possuem uma característica peculiar: capturam a variação conjunta de elementos de um dataset e transforma essa covariância em uma relação hierárquica. Entre as diversas aplicações está a capacidade de clusterizar ações que oscilam juntas. Isto pode ser particularmente interessante para ajudar traders e investidores a formarem uma carteira variada.\n",
    "Para poder ilustrar este exemplo, vamos usar dados de 60 ativos de empresas americanas referente ao período de 2010 a 2015. Primeiro vamos fazer as importações necessárias:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "Para poder processar os nossos dados, vamos precisar extrair os valores do nosso dataframe das variáveis numéricas e transformá-los em uma array. Ao mesmo tempo, vamos precisar extrair o nome das empresas e armazenar em uma lista:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "Os algoritmos de clustering hierárquicos são sensíveis à distância, então para não deixar que eles sejam influenciados por diferentes escalas, mas apenas pelas variações, vamos precisar normalizar os nossos dados:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "Agora conseguimos gerar uma visualização que irá nos dar um entendimento da formação dos clusters hierárquicos:\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "Agora precisamos extrair os clusters para fazer as nossas recomendações.\n",
    "Para isso, precisamos estabelecer o limite de corte da nossa árvore. Na parte de baixo temos todas as empresas ou folhas. Se fizermos um corte em 0.5 por exemplo, teremos 60 clusters. Um para cada empresa. Se fizermos acima de 1.4 teremos apenas um grande cluster com as 60 empresas. Precisamos estabelecer um corte que otimize e capture o que estamos buscando (a covariância dos ativos).\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "<img src = \"https://images2.imgbox.com/18/c0/IRq1mVhl_o.png\" width=\"500\">\n",
    "\n",
    "Agora conseguimos enxergar facilmente os clusters e as empresas que os compõem!\n",
    "\n",
    "E o melhor de tudo, já temos um dataframe com a label de cada cluster e o nome de todas as empresas…\n",
    "\n",
    "Desafio: Crie um nome para cada label que ajude aos investidores a identificar quais empresas estão em um determinado cluster.\n",
    "\n",
    "## Conclusão\n",
    "\n",
    "Que bom que você chegou até aqui!\n",
    "Chegamos ao fim dos estudos básicos de algoritmos não-supervisionados. Durante essa jornada, passamos pelos principais tipos de algoritmos de clusterização passando por exemplos dos mais conhecidos e no final, podemos ver o KMeans e o Cluster Hierárquico em uma aplicação de datasets reais. Estas novas habilidades são fundamentais para a caixa de ferramenta de profissionais de ciência de dados.\n",
    "Com este texto e o de Redução de dimensionalidade, finalizamos o bloco de aprendizado não supervisionado. Como sempre, esperamos que tenha ficado claro cada parte dos nossos textos, mas se a qualquer momento surgir uma dúvida, lembre-se, tem sempre alguém do time Tera pronto para te ajudar!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2a65b",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
